{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN Implementation_less_headache",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6frBpqOXWJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.autograd as autograd         # computation graph\n",
        "from torch import Tensor                  # tensor node in the computation graph\n",
        "import torch.nn as nn                     # neural networks\n",
        "import torch.nn.functional as F           # layers, activations and more\n",
        "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
        "import gym\n",
        "from collections import deque\n",
        "import collections, itertools\n",
        "import numpy as np\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import pickle                             #for saving objects"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRkwL0cW5RPx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cf505cfc-d3cf-441a-c7ec-e48b64db6d9e"
      },
      "source": [
        "#if training with google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive_folder=\"/content/gdrive/My Drive/DQN_save_files/\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BDdN2GxhXcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''    #it took 3000+ episodes to solve with 120 neurons per layer and just 300 episodes with 500 neurons per layer\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self,obs_array_len, num_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc =nn.Linear(obs_array_len,120)\n",
        "        self.fc1= nn.Linear(120,120)\n",
        "        self.output = nn.Linear(120, num_actions) \n",
        "        \n",
        "    def forward(self, obs):\n",
        "        x=F.relu(self.fc(obs))\n",
        "        x=F.relu(self.fc1(x))\n",
        "        x=self.output(x)\n",
        "        return x \n",
        "'''\n",
        "#Trying out more neurons per layer\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self,obs_array_len, num_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc =nn.Linear(obs_array_len,500) \n",
        "        self.fc1= nn.Linear(500,500)\n",
        "        self.output = nn.Linear(500, num_actions) \n",
        "        \n",
        "    def forward(self, obs):\n",
        "        x=F.relu(self.fc(obs))\n",
        "        x=F.relu(self.fc1(x))\n",
        "        x=self.output(x)\n",
        "        return x "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKqUntbphYqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#named tupule creates a 'class' that stores state action reward... we can then create a list or queue to store instances of the class\n",
        "Experience = collections.namedtuple('Experience',['state', 'action', 'reward', 'done', 'state1']) #done is important for when we calculate the losses\n",
        "\n",
        "class Experience_replay(): #experience replay datatype\n",
        "    def __init__(self, REPLAY_SIZE):\n",
        "        self.memory= collections.deque(maxlen = REPLAY_SIZE) #deque with max size given\n",
        "\n",
        "    def insert(self, experience):\n",
        "        self.memory.append(experience)\n",
        "        \n",
        "    def size(self):\n",
        "        return len(self.memory)\n",
        "    \n",
        "    def sample(self, batch_size): #return a batch to calculate loss\n",
        "        random_indexes = np.random.choice(len(self.memory), batch_size, replace = False) #create list of random\n",
        "        \n",
        "        states, actions, rewards, dones, state1s = zip(* [self.memory[index] for index in random_indexes])\n",
        "        \n",
        "        states = np.array(states)                \n",
        "        actions = np.array(actions)                \n",
        "        rewards = np.array(rewards, dtype=np.float32)        \n",
        "        dones = np.array(dones, dtype=np.uint8)        \n",
        "        state1s = np.array(state1s)\n",
        "        \n",
        "        return states, actions, rewards, dones, state1s\n",
        "            \n",
        "def to_torch(i):\n",
        "    return torch.from_numpy(i)   \n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lFu4qhUhauQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tb=SummaryWriter(drive_folder+\"runs/Cartpole/Train\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JP9zDTvhdS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(resume=False):\n",
        "\n",
        "    #enable cuda\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda:0\")\n",
        "    \n",
        "    #hyperparameters\n",
        "\n",
        "    GAMMA=0.99                      #Gamma for bellman approx.\n",
        "    BATCH_SIZE=64                   #size of batch to sample from replay memory\n",
        "    REPLAY_SIZE=10000               #size of replay memory\n",
        "    LR=0.000146 #0.001                       #learning rate\n",
        "    EPSILON_START=1.0               #exploration \n",
        "    EPSILON_FINAL=0.05\n",
        "    EPSILON_DECAY_LENGTH=2000        #was 200\n",
        "    epsilon=EPSILON_START\n",
        "\n",
        "    #make environment\n",
        "    \n",
        "    env = gym.make('CartPole-v1')  \n",
        "    obs=env.reset()\n",
        "    done = False\n",
        "\n",
        "    #misc declarations\n",
        "\n",
        "    avg_reward=collections.deque(maxlen = 100) #init a deque with a size of 100. \n",
        "\n",
        "    count =0\n",
        "    episode_reward=0\n",
        "    done_count=0\n",
        "    mean_reward=0\n",
        "    \n",
        "    #Network\n",
        "    \n",
        "    net= DQN(len(obs),env.action_space.n).to(device)          #define networks\n",
        "    target_net= DQN(len(obs),env.action_space.n).to(device)\n",
        "    target_net.load_state_dict(net.state_dict())              #we want the same parameters\n",
        "    target_net.eval()                                         #we want to eventuall copy the parameters of net, not optimize target_net\n",
        "\n",
        "    #Loss\n",
        "    loss_fn=nn.MSELoss()\n",
        "\n",
        "    #Optimizer\n",
        "    optimizer= optim.Adam(net.parameters(), lr=LR)         #was Adam\n",
        "    \n",
        "    #Replay Memory\n",
        "    mem_buffer = Experience_replay(REPLAY_SIZE)\n",
        "\n",
        "    #If we are resuming training\n",
        "    if (resume == True):\n",
        "      #load Network\n",
        "\n",
        "      checkpoint = torch.load(drive_folder+\"CartPole-v1.pth\")\n",
        "      net.load_state_dict(checkpoint['model_state_dict'])\n",
        "      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      count = checkpoint['epoch']\n",
        "      loss = checkpoint['loss']\n",
        "\n",
        "      target_net.load_state_dict(net.state_dict())\n",
        "\n",
        "      #load saved replay memory\n",
        "\n",
        "      with open(drive_folder+'mem_buffer.deque', 'rb') as mem_buffer_file:\n",
        "        mem_buffer = pickle.load(mem_buffer_file)                     \n",
        "\n",
        "    \n",
        "    while (mem_buffer.size() < REPLAY_SIZE):    #play random actions to fill memory buffer\n",
        "        if done:\n",
        "            obs=env.reset()\n",
        "            done=False\n",
        "        action = env.action_space.sample()\n",
        "        \n",
        "        obs1, reward, done, info = env.step(action)\n",
        "        experience = Experience(obs, action, reward, done, obs1)\n",
        "        obs = obs1\n",
        "        mem_buffer.insert(experience)\n",
        "        \n",
        "    \n",
        "    policy=torch.empty(0)    #initilize empty tensor(no values at all) \n",
        "    target=torch.empty(0)    #can use torch.randn(0) too\n",
        "    \n",
        "#-------------------------------------------------------------------------------------------------------------------------------------------------------#\n",
        "\n",
        "    while True:\n",
        "        \n",
        "        if (count==EPSILON_DECAY_LENGTH):\n",
        "            print(\"we have reached final decay\")\n",
        "        \n",
        "        count+=1\n",
        "        \n",
        "        epsilon = max(EPSILON_FINAL, EPSILON_START - count/EPSILON_DECAY_LENGTH) #decay epsilon\n",
        "        \n",
        "        #np.random.random() returns random floats in the half-open interval [0.0, 1.0). \n",
        "        #with epsilon getting smaller, theres less chance that the random number will be within the range of epsilon\n",
        "  \n",
        "        if np.random.random() < epsilon:   \n",
        "          action = env.action_space.sample()\n",
        "        \n",
        "        else:\n",
        "          action= torch.argmax(net(to_torch(obs).to(device).float())) #output action.\n",
        "          action= action.item()       \n",
        "        \n",
        "        obs1, reward, done, info = env.step(action) \n",
        "\n",
        "        #reward = reward if not done else -reward \n",
        "\n",
        "        experience = Experience(obs, action, reward, done, obs1) #create transition\n",
        "        obs = obs1       #update observation for next time step\n",
        "        mem_buffer.insert(experience) #store transition in memory buffer\n",
        "\n",
        "        episode_reward+=reward\n",
        "\n",
        "        if done:\n",
        "            obs=env.reset()\n",
        "            done=False\n",
        "            done_count+=1 #using this to calculate average reward\n",
        "            avg_reward.append(episode_reward)\n",
        "            print('Episode Reward: ',episode_reward)\n",
        "            episode_reward=0\n",
        "        \n",
        "        #Record training performance to tensorboard---------------Average reward over 100 episodes\n",
        "        \n",
        "        if (count%100==0):  \n",
        "          if (done_count>101): \n",
        "            mean_reward=np.mean(list(itertools.islice(avg_reward, 0, 99)))\n",
        "            tb.add_scalar('Mean Reward', mean_reward, done_count)\n",
        "            print('Mean Reward: ',mean_reward, 'Steps',count,'number of episodes:',done_count,'e:', epsilon)\n",
        "            \n",
        "            if (mean_reward>195):\n",
        "              \n",
        "              PATH= drive_folder+\"CartPole-v1.pth\"\n",
        "              torch.save({\n",
        "                  'epoch':count,\n",
        "                  'model_state_dict': net.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer.state_dict(),\n",
        "                  'loss':loss\n",
        "              },\n",
        "              PATH)\n",
        "          \n",
        "              print(\"We Are DONE Training!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "              break              \n",
        "        \n",
        "        #sample from minibatch\n",
        "        \n",
        "        obs_batch, action_batch, reward_batch,done_batch, obs1_batch= mem_buffer.sample(BATCH_SIZE)\n",
        "        \n",
        "        obs_batch= to_torch(obs_batch).to(device).float()\n",
        "        obs1_batch= to_torch(np.array(obs1_batch)).to(device).float()\n",
        "        reward_batch=to_torch(np.array(reward_batch)).to(device)\n",
        "                \n",
        "\n",
        "        for i in range(len(obs_batch)):\n",
        "            if done_batch[i] == True:\n",
        "                target= torch.cat((target.to(device), torch.unsqueeze(reward_batch[i],0) ),0)#for every reward at index i, concatenate to the end of tensor\n",
        "                \n",
        "            else:\n",
        "                #unsqueeze Returns a new tensor with a dimension of size one inserted at the specified position.\n",
        "                target= torch.cat((target.to(device), torch.unsqueeze(reward_batch[i], 0)+( GAMMA*torch.unsqueeze(torch.max(target_net(obs1_batch[i]).detach()), 0)) ),0).to(device)\n",
        "            policy= torch.cat((policy.to(device), torch.unsqueeze(torch.max(net(obs_batch[i])),0) ), 0)\n",
        " \n",
        "\n",
        "        loss=loss_fn(target,policy)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        tb.add_scalar('Loss Per Step', loss.item(), done_count) #record loss in tensorboard\n",
        "        for param in net.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)              #clip gradients          \n",
        "        optimizer.step()\n",
        "        \n",
        "        policy=torch.empty(0)     #empty tensors to use again in loop\n",
        "        target=torch.empty(0) \n",
        "             \n",
        "        \n",
        "        #take the chance to update the target model\n",
        "        if(done_count%10==0): # update every 10 episodes\n",
        "          target_net.load_state_dict(net.state_dict())\n",
        "\n",
        "        '''\n",
        "        #Save model\n",
        "        PATH= drive_folder+\"CartPole-v1.pth\"\n",
        "               \n",
        "        if(count%10000==0): \n",
        "            torch.save({\n",
        "                'epoch':count,\n",
        "                'model_state_dict': net.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss':loss\n",
        "            },\n",
        "            PATH)\n",
        "\n",
        "            #We also save the transitions deque into a file\n",
        "            with open(drive_folder+'mem_buffer.deque', 'wb') as mem_buffer_file:\n",
        "              pickle.dump(mem_buffer, mem_buffer_file)      \n",
        "        '''\n",
        "                \n",
        "        \n",
        "    env.close()    "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqPSLZN4hgCL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "17775a47-9a73-4192-a656-c09c74e441a7"
      },
      "source": [
        "train(resume = False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode Reward:  10.0\n",
            "Episode Reward:  16.0\n",
            "Episode Reward:  19.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  31.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  16.0\n",
            "Episode Reward:  54.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  49.0\n",
            "Episode Reward:  19.0\n",
            "Episode Reward:  23.0\n",
            "Episode Reward:  19.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  10.0\n",
            "Episode Reward:  19.0\n",
            "Episode Reward:  37.0\n",
            "Episode Reward:  27.0\n",
            "Episode Reward:  20.0\n",
            "Episode Reward:  30.0\n",
            "Episode Reward:  16.0\n",
            "Episode Reward:  42.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  38.0\n",
            "Episode Reward:  10.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  16.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  19.0\n",
            "Episode Reward:  27.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  19.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  27.0\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  8.0\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  28.0\n",
            "Episode Reward:  10.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  9.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  10.0\n",
            "Episode Reward:  21.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  10.0\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  10.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  10.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  10.0\n",
            "Episode Reward:  10.0\n",
            "Episode Reward:  9.0\n",
            "Episode Reward:  22.0\n",
            "Episode Reward:  10.0\n",
            "Episode Reward:  17.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  10.0\n",
            "Episode Reward:  9.0\n",
            "Episode Reward:  10.0\n",
            "Episode Reward:  10.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  10.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  11.0\n",
            "Mean Reward:  15.767676767676768 Steps 1600 number of episodes: 102 e: 0.19999999999999996\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  16.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  13.0\n",
            "Mean Reward:  15.080808080808081 Steps 1700 number of episodes: 109 e: 0.15000000000000002\n",
            "Episode Reward:  10.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  10.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  12.0\n",
            "Mean Reward:  14.393939393939394 Steps 1800 number of episodes: 118 e: 0.09999999999999998\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  11.0\n",
            "Mean Reward:  13.151515151515152 Steps 1900 number of episodes: 126 e: 0.050000000000000044\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  11.0\n",
            "Mean Reward:  12.969696969696969 Steps 2000 number of episodes: 135 e: 0.05\n",
            "we have reached final decay\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  11.0\n",
            "Mean Reward:  12.747474747474747 Steps 2100 number of episodes: 142 e: 0.05\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  11.0\n",
            "Mean Reward:  12.555555555555555 Steps 2200 number of episodes: 150 e: 0.05\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  12.0\n",
            "Mean Reward:  12.424242424242424 Steps 2300 number of episodes: 158 e: 0.05\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  10.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  12.0\n",
            "Mean Reward:  12.494949494949495 Steps 2400 number of episodes: 165 e: 0.05\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  16.0\n",
            "Episode Reward:  16.0\n",
            "Mean Reward:  12.454545454545455 Steps 2500 number of episodes: 172 e: 0.05\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  11.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  15.0\n",
            "Mean Reward:  12.626262626262626 Steps 2600 number of episodes: 180 e: 0.05\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  20.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  16.0\n",
            "Mean Reward:  12.626262626262626 Steps 2700 number of episodes: 187 e: 0.05\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  15.0\n",
            "Mean Reward:  12.858585858585858 Steps 2800 number of episodes: 194 e: 0.05\n",
            "Episode Reward:  16.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  14.0\n",
            "Mean Reward:  13.070707070707071 Steps 2900 number of episodes: 201 e: 0.05\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  17.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  17.0\n",
            "Mean Reward:  13.181818181818182 Steps 3000 number of episodes: 208 e: 0.05\n",
            "Episode Reward:  17.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  16.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  16.0\n",
            "Mean Reward:  13.343434343434344 Steps 3100 number of episodes: 214 e: 0.05\n",
            "Episode Reward:  17.0\n",
            "Episode Reward:  17.0\n",
            "Episode Reward:  16.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  16.0\n",
            "Mean Reward:  13.595959595959595 Steps 3200 number of episodes: 220 e: 0.05\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  16.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  17.0\n",
            "Mean Reward:  13.909090909090908 Steps 3300 number of episodes: 227 e: 0.05\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  16.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  12.0\n",
            "Mean Reward:  14.121212121212121 Steps 3400 number of episodes: 233 e: 0.05\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  16.0\n",
            "Episode Reward:  17.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  14.0\n",
            "Mean Reward:  14.282828282828282 Steps 3500 number of episodes: 239 e: 0.05\n",
            "Episode Reward:  19.0\n",
            "Episode Reward:  16.0\n",
            "Episode Reward:  16.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  19.0\n",
            "Episode Reward:  13.0\n",
            "Mean Reward:  14.515151515151516 Steps 3600 number of episodes: 245 e: 0.05\n",
            "Episode Reward:  20.0\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  20.0\n",
            "Episode Reward:  16.0\n",
            "Episode Reward:  20.0\n",
            "Episode Reward:  19.0\n",
            "Mean Reward:  14.818181818181818 Steps 3700 number of episodes: 251 e: 0.05\n",
            "Episode Reward:  16.0\n",
            "Episode Reward:  19.0\n",
            "Episode Reward:  17.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  16.0\n",
            "Mean Reward:  15.070707070707071 Steps 3800 number of episodes: 257 e: 0.05\n",
            "Episode Reward:  20.0\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  20.0\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  18.0\n",
            "Mean Reward:  15.343434343434344 Steps 3900 number of episodes: 263 e: 0.05\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  12.0\n",
            "Episode Reward:  16.0\n",
            "Episode Reward:  17.0\n",
            "Episode Reward:  19.0\n",
            "Episode Reward:  14.0\n",
            "Mean Reward:  15.545454545454545 Steps 4000 number of episodes: 269 e: 0.05\n",
            "Episode Reward:  17.0\n",
            "Episode Reward:  16.0\n",
            "Episode Reward:  16.0\n",
            "Episode Reward:  17.0\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  16.0\n",
            "Mean Reward:  15.696969696969697 Steps 4100 number of episodes: 275 e: 0.05\n",
            "Episode Reward:  19.0\n",
            "Episode Reward:  22.0\n",
            "Episode Reward:  19.0\n",
            "Episode Reward:  15.0\n",
            "Episode Reward:  16.0\n",
            "Mean Reward:  15.929292929292929 Steps 4200 number of episodes: 280 e: 0.05\n",
            "Episode Reward:  14.0\n",
            "Episode Reward:  13.0\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  16.0\n",
            "Episode Reward:  20.0\n",
            "Mean Reward:  16.03030303030303 Steps 4300 number of episodes: 286 e: 0.05\n",
            "Episode Reward:  19.0\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  17.0\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  25.0\n",
            "Mean Reward:  16.2020202020202 Steps 4400 number of episodes: 291 e: 0.05\n",
            "Episode Reward:  26.0\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  22.0\n",
            "Episode Reward:  24.0\n",
            "Episode Reward:  20.0\n",
            "Mean Reward:  16.666666666666668 Steps 4500 number of episodes: 296 e: 0.05\n",
            "Episode Reward:  23.0\n",
            "Episode Reward:  19.0\n",
            "Episode Reward:  20.0\n",
            "Episode Reward:  22.0\n",
            "Episode Reward:  23.0\n",
            "Mean Reward:  16.98989898989899 Steps 4600 number of episodes: 301 e: 0.05\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  20.0\n",
            "Episode Reward:  22.0\n",
            "Episode Reward:  17.0\n",
            "Episode Reward:  16.0\n",
            "Mean Reward:  17.252525252525253 Steps 4700 number of episodes: 306 e: 0.05\n",
            "Episode Reward:  23.0\n",
            "Episode Reward:  24.0\n",
            "Episode Reward:  22.0\n",
            "Episode Reward:  19.0\n",
            "Mean Reward:  17.484848484848484 Steps 4800 number of episodes: 310 e: 0.05\n",
            "Episode Reward:  22.0\n",
            "Episode Reward:  27.0\n",
            "Episode Reward:  18.0\n",
            "Episode Reward:  24.0\n",
            "Episode Reward:  28.0\n",
            "Mean Reward:  17.848484848484848 Steps 4900 number of episodes: 315 e: 0.05\n",
            "Episode Reward:  24.0\n",
            "Episode Reward:  23.0\n",
            "Episode Reward:  21.0\n",
            "Episode Reward:  21.0\n",
            "Mean Reward:  18.181818181818183 Steps 5000 number of episodes: 319 e: 0.05\n",
            "Episode Reward:  22.0\n",
            "Episode Reward:  23.0\n",
            "Episode Reward:  27.0\n",
            "Episode Reward:  25.0\n",
            "Mean Reward:  18.464646464646464 Steps 5100 number of episodes: 323 e: 0.05\n",
            "Episode Reward:  27.0\n",
            "Episode Reward:  25.0\n",
            "Episode Reward:  20.0\n",
            "Episode Reward:  36.0\n",
            "Mean Reward:  18.77777777777778 Steps 5200 number of episodes: 327 e: 0.05\n",
            "Episode Reward:  32.0\n",
            "Episode Reward:  25.0\n",
            "Episode Reward:  23.0\n",
            "Mean Reward:  19.272727272727273 Steps 5300 number of episodes: 330 e: 0.05\n",
            "Episode Reward:  30.0\n",
            "Episode Reward:  27.0\n",
            "Episode Reward:  28.0\n",
            "Episode Reward:  33.0\n",
            "Mean Reward:  19.747474747474747 Steps 5400 number of episodes: 334 e: 0.05\n",
            "Episode Reward:  85.0\n",
            "Mean Reward:  19.8989898989899 Steps 5500 number of episodes: 335 e: 0.05\n",
            "Episode Reward:  32.0\n",
            "Episode Reward:  78.0\n",
            "Mean Reward:  20.747474747474747 Steps 5600 number of episodes: 337 e: 0.05\n",
            "Episode Reward:  67.0\n",
            "Mean Reward:  21.393939393939394 Steps 5700 number of episodes: 338 e: 0.05\n",
            "Episode Reward:  77.0\n",
            "Mean Reward:  21.92929292929293 Steps 5800 number of episodes: 339 e: 0.05\n",
            "Episode Reward:  78.0\n",
            "Episode Reward:  77.0\n",
            "Mean Reward:  23.141414141414142 Steps 5900 number of episodes: 341 e: 0.05\n",
            "Episode Reward:  72.0\n",
            "Mean Reward:  23.757575757575758 Steps 6000 number of episodes: 342 e: 0.05\n",
            "Episode Reward:  69.0\n",
            "Mean Reward:  24.333333333333332 Steps 6100 number of episodes: 343 e: 0.05\n",
            "Episode Reward:  76.0\n",
            "Episode Reward:  75.0\n",
            "Mean Reward:  25.474747474747474 Steps 6200 number of episodes: 345 e: 0.05\n",
            "Episode Reward:  77.0\n",
            "Mean Reward:  26.03030303030303 Steps 6300 number of episodes: 346 e: 0.05\n",
            "Episode Reward:  82.0\n",
            "Mean Reward:  26.626262626262626 Steps 6400 number of episodes: 347 e: 0.05\n",
            "Episode Reward:  69.0\n",
            "Episode Reward:  86.0\n",
            "Mean Reward:  27.78787878787879 Steps 6500 number of episodes: 349 e: 0.05\n",
            "Episode Reward:  72.0\n",
            "Mean Reward:  28.454545454545453 Steps 6600 number of episodes: 350 e: 0.05\n",
            "Episode Reward:  60.0\n",
            "Mean Reward:  28.98989898989899 Steps 6700 number of episodes: 351 e: 0.05\n",
            "Episode Reward:  83.0\n",
            "Episode Reward:  85.0\n",
            "Mean Reward:  30.08080808080808 Steps 6800 number of episodes: 353 e: 0.05\n",
            "Episode Reward:  80.0\n",
            "Mean Reward:  30.767676767676768 Steps 6900 number of episodes: 354 e: 0.05\n",
            "Episode Reward:  80.0\n",
            "Mean Reward:  31.424242424242426 Steps 7000 number of episodes: 355 e: 0.05\n",
            "Episode Reward:  75.0\n",
            "Mean Reward:  32.05050505050505 Steps 7100 number of episodes: 356 e: 0.05\n",
            "Episode Reward:  95.0\n",
            "Mean Reward:  32.64646464646464 Steps 7200 number of episodes: 357 e: 0.05\n",
            "Episode Reward:  99.0\n",
            "Episode Reward:  76.0\n",
            "Mean Reward:  34.22222222222222 Steps 7300 number of episodes: 359 e: 0.05\n",
            "Mean Reward:  34.22222222222222 Steps 7400 number of episodes: 359 e: 0.05\n",
            "Episode Reward:  128.0\n",
            "Mean Reward:  34.78787878787879 Steps 7500 number of episodes: 360 e: 0.05\n",
            "Episode Reward:  77.0\n",
            "Episode Reward:  88.0\n",
            "Mean Reward:  36.56565656565657 Steps 7600 number of episodes: 362 e: 0.05\n",
            "Episode Reward:  108.0\n",
            "Mean Reward:  37.27272727272727 Steps 7700 number of episodes: 363 e: 0.05\n",
            "Mean Reward:  37.27272727272727 Steps 7800 number of episodes: 363 e: 0.05\n",
            "Episode Reward:  129.0\n",
            "Mean Reward:  38.18181818181818 Steps 7900 number of episodes: 364 e: 0.05\n",
            "Episode Reward:  130.0\n",
            "Mean Reward:  39.36363636363637 Steps 8000 number of episodes: 365 e: 0.05\n",
            "Episode Reward:  126.0\n",
            "Mean Reward:  40.515151515151516 Steps 8100 number of episodes: 366 e: 0.05\n",
            "Mean Reward:  40.515151515151516 Steps 8200 number of episodes: 366 e: 0.05\n",
            "Episode Reward:  133.0\n",
            "Mean Reward:  41.61616161616162 Steps 8300 number of episodes: 367 e: 0.05\n",
            "Episode Reward:  147.0\n",
            "Mean Reward:  42.76767676767677 Steps 8400 number of episodes: 368 e: 0.05\n",
            "Episode Reward:  125.0\n",
            "Mean Reward:  44.111111111111114 Steps 8500 number of episodes: 369 e: 0.05\n",
            "Mean Reward:  44.111111111111114 Steps 8600 number of episodes: 369 e: 0.05\n",
            "Episode Reward:  149.0\n",
            "Mean Reward:  45.2020202020202 Steps 8700 number of episodes: 370 e: 0.05\n",
            "Episode Reward:  104.0\n",
            "Mean Reward:  46.54545454545455 Steps 8800 number of episodes: 371 e: 0.05\n",
            "Mean Reward:  46.54545454545455 Steps 8900 number of episodes: 371 e: 0.05\n",
            "Episode Reward:  188.0\n",
            "Mean Reward:  47.43434343434343 Steps 9000 number of episodes: 372 e: 0.05\n",
            "Mean Reward:  47.43434343434343 Steps 9100 number of episodes: 372 e: 0.05\n",
            "Episode Reward:  184.0\n",
            "Mean Reward:  49.16161616161616 Steps 9200 number of episodes: 373 e: 0.05\n",
            "Mean Reward:  49.16161616161616 Steps 9300 number of episodes: 373 e: 0.05\n",
            "Episode Reward:  201.0\n",
            "Mean Reward:  50.83838383838384 Steps 9400 number of episodes: 374 e: 0.05\n",
            "Episode Reward:  157.0\n",
            "Mean Reward:  52.707070707070706 Steps 9500 number of episodes: 375 e: 0.05\n",
            "Mean Reward:  52.707070707070706 Steps 9600 number of episodes: 375 e: 0.05\n",
            "Episode Reward:  131.0\n",
            "Mean Reward:  54.101010101010104 Steps 9700 number of episodes: 376 e: 0.05\n",
            "Episode Reward:  109.0\n",
            "Mean Reward:  55.2020202020202 Steps 9800 number of episodes: 377 e: 0.05\n",
            "Mean Reward:  55.2020202020202 Steps 9900 number of episodes: 377 e: 0.05\n",
            "Episode Reward:  250.0\n",
            "Mean Reward:  56.111111111111114 Steps 10000 number of episodes: 378 e: 0.05\n",
            "Mean Reward:  56.111111111111114 Steps 10100 number of episodes: 378 e: 0.05\n",
            "Episode Reward:  150.0\n",
            "Mean Reward:  58.484848484848484 Steps 10200 number of episodes: 379 e: 0.05\n",
            "Mean Reward:  58.484848484848484 Steps 10300 number of episodes: 379 e: 0.05\n",
            "Episode Reward:  212.0\n",
            "Mean Reward:  59.83838383838384 Steps 10400 number of episodes: 380 e: 0.05\n",
            "Mean Reward:  59.83838383838384 Steps 10500 number of episodes: 380 e: 0.05\n",
            "Episode Reward:  216.0\n",
            "Mean Reward:  61.83838383838384 Steps 10600 number of episodes: 381 e: 0.05\n",
            "Mean Reward:  61.83838383838384 Steps 10700 number of episodes: 381 e: 0.05\n",
            "Mean Reward:  61.83838383838384 Steps 10800 number of episodes: 381 e: 0.05\n",
            "Mean Reward:  61.83838383838384 Steps 10900 number of episodes: 381 e: 0.05\n",
            "Episode Reward:  423.0\n",
            "Mean Reward:  63.888888888888886 Steps 11000 number of episodes: 382 e: 0.05\n",
            "Mean Reward:  63.888888888888886 Steps 11100 number of episodes: 382 e: 0.05\n",
            "Episode Reward:  202.0\n",
            "Mean Reward:  67.97979797979798 Steps 11200 number of episodes: 383 e: 0.05\n",
            "Episode Reward:  120.0\n",
            "Mean Reward:  69.83838383838383 Steps 11300 number of episodes: 384 e: 0.05\n",
            "Episode Reward:  108.0\n",
            "Mean Reward:  70.88888888888889 Steps 11400 number of episodes: 385 e: 0.05\n",
            "Mean Reward:  70.88888888888889 Steps 11500 number of episodes: 385 e: 0.05\n",
            "Episode Reward:  186.0\n",
            "Mean Reward:  71.77777777777777 Steps 11600 number of episodes: 386 e: 0.05\n",
            "Mean Reward:  71.77777777777777 Steps 11700 number of episodes: 386 e: 0.05\n",
            "Mean Reward:  71.77777777777777 Steps 11800 number of episodes: 386 e: 0.05\n",
            "Episode Reward:  226.0\n",
            "Mean Reward:  73.46464646464646 Steps 11900 number of episodes: 387 e: 0.05\n",
            "Episode Reward:  192.0\n",
            "Mean Reward:  75.56565656565657 Steps 12000 number of episodes: 388 e: 0.05\n",
            "Mean Reward:  75.56565656565657 Steps 12100 number of episodes: 388 e: 0.05\n",
            "Episode Reward:  137.0\n",
            "Mean Reward:  77.33333333333333 Steps 12200 number of episodes: 389 e: 0.05\n",
            "Episode Reward:  94.0\n",
            "Mean Reward:  78.53535353535354 Steps 12300 number of episodes: 390 e: 0.05\n",
            "Episode Reward:  86.0\n",
            "Mean Reward:  79.23232323232324 Steps 12400 number of episodes: 391 e: 0.05\n",
            "Episode Reward:  137.0\n",
            "Mean Reward:  79.83838383838383 Steps 12500 number of episodes: 392 e: 0.05\n",
            "Mean Reward:  79.83838383838383 Steps 12600 number of episodes: 392 e: 0.05\n",
            "Episode Reward:  188.0\n",
            "Mean Reward:  81.04040404040404 Steps 12700 number of episodes: 393 e: 0.05\n",
            "Episode Reward:  132.0\n",
            "Mean Reward:  82.71717171717172 Steps 12800 number of episodes: 394 e: 0.05\n",
            "Mean Reward:  82.71717171717172 Steps 12900 number of episodes: 394 e: 0.05\n",
            "Episode Reward:  165.0\n",
            "Mean Reward:  83.8080808080808 Steps 13000 number of episodes: 395 e: 0.05\n",
            "Mean Reward:  83.8080808080808 Steps 13100 number of episodes: 395 e: 0.05\n",
            "Mean Reward:  83.8080808080808 Steps 13200 number of episodes: 395 e: 0.05\n",
            "Mean Reward:  83.8080808080808 Steps 13300 number of episodes: 395 e: 0.05\n",
            "Mean Reward:  83.8080808080808 Steps 13400 number of episodes: 395 e: 0.05\n",
            "Episode Reward:  491.0\n",
            "Mean Reward:  85.27272727272727 Steps 13500 number of episodes: 396 e: 0.05\n",
            "Episode Reward:  158.0\n",
            "Mean Reward:  90.0 Steps 13600 number of episodes: 397 e: 0.05\n",
            "Mean Reward:  90.0 Steps 13700 number of episodes: 397 e: 0.05\n",
            "Episode Reward:  158.0\n",
            "Mean Reward:  91.4040404040404 Steps 13800 number of episodes: 398 e: 0.05\n",
            "Mean Reward:  91.4040404040404 Steps 13900 number of episodes: 398 e: 0.05\n",
            "Episode Reward:  169.0\n",
            "Mean Reward:  92.79797979797979 Steps 14000 number of episodes: 399 e: 0.05\n",
            "Episode Reward:  182.0\n",
            "Mean Reward:  94.28282828282828 Steps 14100 number of episodes: 400 e: 0.05\n",
            "Mean Reward:  94.28282828282828 Steps 14200 number of episodes: 400 e: 0.05\n",
            "Mean Reward:  94.28282828282828 Steps 14300 number of episodes: 400 e: 0.05\n",
            "Episode Reward:  212.0\n",
            "Mean Reward:  95.88888888888889 Steps 14400 number of episodes: 401 e: 0.05\n",
            "Episode Reward:  128.0\n",
            "Mean Reward:  97.84848484848484 Steps 14500 number of episodes: 402 e: 0.05\n",
            "Episode Reward:  148.0\n",
            "Mean Reward:  98.93939393939394 Steps 14600 number of episodes: 403 e: 0.05\n",
            "Episode Reward:  117.0\n",
            "Mean Reward:  100.21212121212122 Steps 14700 number of episodes: 404 e: 0.05\n",
            "Episode Reward:  87.0\n",
            "Mean Reward:  101.22222222222223 Steps 14800 number of episodes: 405 e: 0.05\n",
            "Episode Reward:  90.0\n",
            "Mean Reward:  101.93939393939394 Steps 14900 number of episodes: 406 e: 0.05\n",
            "Episode Reward:  75.0\n",
            "Mean Reward:  102.61616161616162 Steps 15000 number of episodes: 407 e: 0.05\n",
            "Episode Reward:  121.0\n",
            "Mean Reward:  103.13131313131314 Steps 15100 number of episodes: 408 e: 0.05\n",
            "Mean Reward:  103.13131313131314 Steps 15200 number of episodes: 408 e: 0.05\n",
            "Episode Reward:  153.0\n",
            "Mean Reward:  104.13131313131314 Steps 15300 number of episodes: 409 e: 0.05\n",
            "Mean Reward:  104.13131313131314 Steps 15400 number of episodes: 409 e: 0.05\n",
            "Episode Reward:  208.0\n",
            "Mean Reward:  105.48484848484848 Steps 15500 number of episodes: 410 e: 0.05\n",
            "Mean Reward:  105.48484848484848 Steps 15600 number of episodes: 410 e: 0.05\n",
            "Episode Reward:  252.0\n",
            "Mean Reward:  107.36363636363636 Steps 15700 number of episodes: 411 e: 0.05\n",
            "Mean Reward:  107.36363636363636 Steps 15800 number of episodes: 411 e: 0.05\n",
            "Episode Reward:  126.0\n",
            "Episode Reward:  90.0\n",
            "Mean Reward:  110.72727272727273 Steps 15900 number of episodes: 413 e: 0.05\n",
            "Episode Reward:  86.0\n",
            "Mean Reward:  111.39393939393939 Steps 16000 number of episodes: 414 e: 0.05\n",
            "Mean Reward:  111.39393939393939 Steps 16100 number of episodes: 414 e: 0.05\n",
            "Episode Reward:  175.0\n",
            "Mean Reward:  111.97979797979798 Steps 16200 number of episodes: 415 e: 0.05\n",
            "Episode Reward:  105.0\n",
            "Mean Reward:  113.5050505050505 Steps 16300 number of episodes: 416 e: 0.05\n",
            "Mean Reward:  113.5050505050505 Steps 16400 number of episodes: 416 e: 0.05\n",
            "Episode Reward:  176.0\n",
            "Mean Reward:  114.33333333333333 Steps 16500 number of episodes: 417 e: 0.05\n",
            "Episode Reward:  82.0\n",
            "Mean Reward:  115.8989898989899 Steps 16600 number of episodes: 418 e: 0.05\n",
            "Episode Reward:  167.0\n",
            "Mean Reward:  116.51515151515152 Steps 16700 number of episodes: 419 e: 0.05\n",
            "Mean Reward:  116.51515151515152 Steps 16800 number of episodes: 419 e: 0.05\n",
            "Episode Reward:  139.0\n",
            "Mean Reward:  117.97979797979798 Steps 16900 number of episodes: 420 e: 0.05\n",
            "Episode Reward:  77.0\n",
            "Mean Reward:  119.15151515151516 Steps 17000 number of episodes: 421 e: 0.05\n",
            "Episode Reward:  112.0\n",
            "Episode Reward:  79.0\n",
            "Mean Reward:  120.53535353535354 Steps 17100 number of episodes: 423 e: 0.05\n",
            "Mean Reward:  120.53535353535354 Steps 17200 number of episodes: 423 e: 0.05\n",
            "Episode Reward:  138.0\n",
            "Mean Reward:  121.06060606060606 Steps 17300 number of episodes: 424 e: 0.05\n",
            "Episode Reward:  109.0\n",
            "Mean Reward:  122.20202020202021 Steps 17400 number of episodes: 425 e: 0.05\n",
            "Episode Reward:  110.0\n",
            "Mean Reward:  123.1010101010101 Steps 17500 number of episodes: 426 e: 0.05\n",
            "Episode Reward:  132.0\n",
            "Mean Reward:  123.84848484848484 Steps 17600 number of episodes: 427 e: 0.05\n",
            "Episode Reward:  112.0\n",
            "Mean Reward:  124.85858585858585 Steps 17700 number of episodes: 428 e: 0.05\n",
            "Episode Reward:  100.0\n",
            "Mean Reward:  125.73737373737374 Steps 17800 number of episodes: 429 e: 0.05\n",
            "Mean Reward:  125.73737373737374 Steps 17900 number of episodes: 429 e: 0.05\n",
            "Episode Reward:  120.0\n",
            "Mean Reward:  126.51515151515152 Steps 18000 number of episodes: 430 e: 0.05\n",
            "Episode Reward:  86.0\n",
            "Episode Reward:  94.0\n",
            "Mean Reward:  128.02020202020202 Steps 18100 number of episodes: 432 e: 0.05\n",
            "Episode Reward:  88.0\n",
            "Mean Reward:  128.68686868686868 Steps 18200 number of episodes: 433 e: 0.05\n",
            "Episode Reward:  80.0\n",
            "Mean Reward:  129.24242424242425 Steps 18300 number of episodes: 434 e: 0.05\n",
            "Episode Reward:  105.0\n",
            "Mean Reward:  129.1919191919192 Steps 18400 number of episodes: 435 e: 0.05\n",
            "Episode Reward:  79.0\n",
            "Mean Reward:  129.92929292929293 Steps 18500 number of episodes: 436 e: 0.05\n",
            "Episode Reward:  86.0\n",
            "Mean Reward:  129.93939393939394 Steps 18600 number of episodes: 437 e: 0.05\n",
            "Mean Reward:  129.93939393939394 Steps 18700 number of episodes: 437 e: 0.05\n",
            "Episode Reward:  254.0\n",
            "Mean Reward:  130.13131313131314 Steps 18800 number of episodes: 438 e: 0.05\n",
            "Episode Reward:  76.0\n",
            "Mean Reward:  131.91919191919192 Steps 18900 number of episodes: 439 e: 0.05\n",
            "Mean Reward:  131.91919191919192 Steps 19000 number of episodes: 439 e: 0.05\n",
            "Episode Reward:  213.0\n",
            "Mean Reward:  131.8989898989899 Steps 19100 number of episodes: 440 e: 0.05\n",
            "Episode Reward:  74.0\n",
            "Mean Reward:  133.27272727272728 Steps 19200 number of episodes: 441 e: 0.05\n",
            "Mean Reward:  133.27272727272728 Steps 19300 number of episodes: 441 e: 0.05\n",
            "Episode Reward:  168.0\n",
            "Episode Reward:  64.0\n",
            "Mean Reward:  134.2929292929293 Steps 19400 number of episodes: 443 e: 0.05\n",
            "Episode Reward:  67.0\n",
            "Mean Reward:  134.17171717171718 Steps 19500 number of episodes: 444 e: 0.05\n",
            "Episode Reward:  78.0\n",
            "Mean Reward:  134.0909090909091 Steps 19600 number of episodes: 445 e: 0.05\n",
            "Episode Reward:  95.0\n",
            "Mean Reward:  134.1010101010101 Steps 19700 number of episodes: 446 e: 0.05\n",
            "Episode Reward:  105.0\n",
            "Mean Reward:  134.23232323232324 Steps 19800 number of episodes: 447 e: 0.05\n",
            "Episode Reward:  84.0\n",
            "Episode Reward:  66.0\n",
            "Mean Reward:  134.57575757575756 Steps 19900 number of episodes: 449 e: 0.05\n",
            "Episode Reward:  54.0\n",
            "Mean Reward:  134.5151515151515 Steps 20000 number of episodes: 450 e: 0.05\n",
            "Episode Reward:  81.0\n",
            "Mean Reward:  134.45454545454547 Steps 20100 number of episodes: 451 e: 0.05\n",
            "Episode Reward:  105.0\n",
            "Episode Reward:  47.0\n",
            "Mean Reward:  134.63636363636363 Steps 20200 number of episodes: 453 e: 0.05\n",
            "Episode Reward:  55.0\n",
            "Mean Reward:  134.3030303030303 Steps 20300 number of episodes: 454 e: 0.05\n",
            "Episode Reward:  93.0\n",
            "Mean Reward:  134.05050505050505 Steps 20400 number of episodes: 455 e: 0.05\n",
            "Episode Reward:  158.0\n",
            "Mean Reward:  134.23232323232324 Steps 20500 number of episodes: 456 e: 0.05\n",
            "Episode Reward:  73.0\n",
            "Mean Reward:  134.86868686868686 Steps 20600 number of episodes: 457 e: 0.05\n",
            "Episode Reward:  98.0\n",
            "Mean Reward:  134.6060606060606 Steps 20700 number of episodes: 458 e: 0.05\n",
            "Episode Reward:  112.0\n",
            "Mean Reward:  134.82828282828282 Steps 20800 number of episodes: 459 e: 0.05\n",
            "Episode Reward:  107.0\n",
            "Mean Reward:  134.66666666666666 Steps 20900 number of episodes: 460 e: 0.05\n",
            "Episode Reward:  91.0\n",
            "Mean Reward:  134.96969696969697 Steps 21000 number of episodes: 461 e: 0.05\n",
            "Episode Reward:  51.0\n",
            "Episode Reward:  92.0\n",
            "Mean Reward:  134.42424242424244 Steps 21100 number of episodes: 463 e: 0.05\n",
            "Episode Reward:  62.0\n",
            "Mean Reward:  134.05050505050505 Steps 21200 number of episodes: 464 e: 0.05\n",
            "Episode Reward:  69.0\n",
            "Mean Reward:  133.36363636363637 Steps 21300 number of episodes: 465 e: 0.05\n",
            "Episode Reward:  114.0\n",
            "Mean Reward:  132.78787878787878 Steps 21400 number of episodes: 466 e: 0.05\n",
            "Mean Reward:  132.78787878787878 Steps 21500 number of episodes: 466 e: 0.05\n",
            "Episode Reward:  177.0\n",
            "Mean Reward:  132.59595959595958 Steps 21600 number of episodes: 467 e: 0.05\n",
            "Episode Reward:  92.0\n",
            "Mean Reward:  132.8989898989899 Steps 21700 number of episodes: 468 e: 0.05\n",
            "Episode Reward:  144.0\n",
            "Mean Reward:  132.56565656565655 Steps 21800 number of episodes: 469 e: 0.05\n",
            "Episode Reward:  124.0\n",
            "Mean Reward:  132.5151515151515 Steps 21900 number of episodes: 470 e: 0.05\n",
            "Episode Reward:  85.0\n",
            "Mean Reward:  132.7171717171717 Steps 22000 number of episodes: 471 e: 0.05\n",
            "Episode Reward:  71.0\n",
            "Mean Reward:  131.67676767676767 Steps 22100 number of episodes: 472 e: 0.05\n",
            "Episode Reward:  70.0\n",
            "Episode Reward:  94.0\n",
            "Mean Reward:  129.21212121212122 Steps 22200 number of episodes: 474 e: 0.05\n",
            "Episode Reward:  99.0\n",
            "Mean Reward:  128.57575757575756 Steps 22300 number of episodes: 475 e: 0.05\n",
            "Episode Reward:  65.0\n",
            "Mean Reward:  128.25252525252526 Steps 22400 number of episodes: 476 e: 0.05\n",
            "Episode Reward:  109.0\n",
            "Mean Reward:  127.8080808080808 Steps 22500 number of episodes: 477 e: 0.05\n",
            "Episode Reward:  76.0\n",
            "Mean Reward:  126.38383838383838 Steps 22600 number of episodes: 478 e: 0.05\n",
            "Episode Reward:  86.0\n",
            "Mean Reward:  125.63636363636364 Steps 22700 number of episodes: 479 e: 0.05\n",
            "Episode Reward:  100.0\n",
            "Mean Reward:  124.36363636363636 Steps 22800 number of episodes: 480 e: 0.05\n",
            "Episode Reward:  117.0\n",
            "Mean Reward:  123.1919191919192 Steps 22900 number of episodes: 481 e: 0.05\n",
            "Episode Reward:  71.0\n",
            "Episode Reward:  68.0\n",
            "Mean Reward:  118.77777777777777 Steps 23000 number of episodes: 483 e: 0.05\n",
            "Episode Reward:  92.0\n",
            "Mean Reward:  118.25252525252525 Steps 23100 number of episodes: 484 e: 0.05\n",
            "Episode Reward:  117.0\n",
            "Mean Reward:  118.0909090909091 Steps 23200 number of episodes: 485 e: 0.05\n",
            "Episode Reward:  94.0\n",
            "Mean Reward:  117.39393939393939 Steps 23300 number of episodes: 486 e: 0.05\n",
            "Episode Reward:  87.0\n",
            "Mean Reward:  116.06060606060606 Steps 23400 number of episodes: 487 e: 0.05\n",
            "Episode Reward:  71.0\n",
            "Mean Reward:  115.0 Steps 23500 number of episodes: 488 e: 0.05\n",
            "Episode Reward:  96.0\n",
            "Mean Reward:  114.33333333333333 Steps 23600 number of episodes: 489 e: 0.05\n",
            "Episode Reward:  94.0\n",
            "Mean Reward:  114.35353535353535 Steps 23700 number of episodes: 490 e: 0.05\n",
            "Episode Reward:  72.0\n",
            "Mean Reward:  114.43434343434343 Steps 23800 number of episodes: 491 e: 0.05\n",
            "Episode Reward:  88.0\n",
            "Episode Reward:  55.0\n",
            "Mean Reward:  112.76767676767676 Steps 23900 number of episodes: 493 e: 0.05\n",
            "Episode Reward:  98.0\n",
            "Mean Reward:  111.98989898989899 Steps 24000 number of episodes: 494 e: 0.05\n",
            "Episode Reward:  83.0\n",
            "Mean Reward:  111.31313131313131 Steps 24100 number of episodes: 495 e: 0.05\n",
            "Episode Reward:  95.0\n",
            "Mean Reward:  107.1919191919192 Steps 24200 number of episodes: 496 e: 0.05\n",
            "Episode Reward:  78.0\n",
            "Mean Reward:  106.55555555555556 Steps 24300 number of episodes: 497 e: 0.05\n",
            "Mean Reward:  106.55555555555556 Steps 24400 number of episodes: 497 e: 0.05\n",
            "Episode Reward:  209.0\n",
            "Mean Reward:  105.74747474747475 Steps 24500 number of episodes: 498 e: 0.05\n",
            "Episode Reward:  106.0\n",
            "Mean Reward:  106.15151515151516 Steps 24600 number of episodes: 499 e: 0.05\n",
            "Episode Reward:  106.0\n",
            "Episode Reward:  67.0\n",
            "Mean Reward:  104.31313131313131 Steps 24700 number of episodes: 501 e: 0.05\n",
            "Episode Reward:  84.0\n",
            "Mean Reward:  103.6969696969697 Steps 24800 number of episodes: 502 e: 0.05\n",
            "Episode Reward:  79.0\n",
            "Mean Reward:  103.05050505050505 Steps 24900 number of episodes: 503 e: 0.05\n",
            "Episode Reward:  93.0\n",
            "Mean Reward:  102.66666666666667 Steps 25000 number of episodes: 504 e: 0.05\n",
            "Episode Reward:  67.0\n",
            "Mean Reward:  102.72727272727273 Steps 25100 number of episodes: 505 e: 0.05\n",
            "Episode Reward:  80.0\n",
            "Episode Reward:  89.0\n",
            "Mean Reward:  102.54545454545455 Steps 25200 number of episodes: 507 e: 0.05\n",
            "Mean Reward:  102.54545454545455 Steps 25300 number of episodes: 507 e: 0.05\n",
            "Episode Reward:  119.0\n",
            "Mean Reward:  102.22222222222223 Steps 25400 number of episodes: 508 e: 0.05\n",
            "Episode Reward:  155.0\n",
            "Mean Reward:  101.87878787878788 Steps 25500 number of episodes: 509 e: 0.05\n",
            "Mean Reward:  101.87878787878788 Steps 25600 number of episodes: 509 e: 0.05\n",
            "Episode Reward:  150.0\n",
            "Episode Reward:  80.0\n",
            "Mean Reward:  100.31313131313131 Steps 25700 number of episodes: 511 e: 0.05\n",
            "Episode Reward:  91.0\n",
            "Mean Reward:  99.84848484848484 Steps 25800 number of episodes: 512 e: 0.05\n",
            "Episode Reward:  91.0\n",
            "Mean Reward:  99.85858585858585 Steps 25900 number of episodes: 513 e: 0.05\n",
            "Episode Reward:  95.0\n",
            "Mean Reward:  99.9090909090909 Steps 26000 number of episodes: 514 e: 0.05\n",
            "Episode Reward:  75.0\n",
            "Mean Reward:  99.1010101010101 Steps 26100 number of episodes: 515 e: 0.05\n",
            "Episode Reward:  91.0\n",
            "Mean Reward:  98.79797979797979 Steps 26200 number of episodes: 516 e: 0.05\n",
            "Episode Reward:  92.0\n",
            "Mean Reward:  97.93939393939394 Steps 26300 number of episodes: 517 e: 0.05\n",
            "Episode Reward:  84.0\n",
            "Episode Reward:  80.0\n",
            "Mean Reward:  97.20202020202021 Steps 26400 number of episodes: 519 e: 0.05\n",
            "Episode Reward:  66.0\n",
            "Mean Reward:  96.60606060606061 Steps 26500 number of episodes: 520 e: 0.05\n",
            "Mean Reward:  96.60606060606061 Steps 26600 number of episodes: 520 e: 0.05\n",
            "Episode Reward:  151.0\n",
            "Episode Reward:  67.0\n",
            "Mean Reward:  96.88888888888889 Steps 26700 number of episodes: 522 e: 0.05\n",
            "Mean Reward:  96.88888888888889 Steps 26800 number of episodes: 522 e: 0.05\n",
            "Mean Reward:  96.88888888888889 Steps 26900 number of episodes: 522 e: 0.05\n",
            "Episode Reward:  224.0\n",
            "Mean Reward:  96.76767676767676 Steps 27000 number of episodes: 523 e: 0.05\n",
            "Episode Reward:  106.0\n",
            "Episode Reward:  72.0\n",
            "Mean Reward:  97.60606060606061 Steps 27100 number of episodes: 525 e: 0.05\n",
            "Episode Reward:  120.0\n",
            "Mean Reward:  97.22222222222223 Steps 27200 number of episodes: 526 e: 0.05\n",
            "Episode Reward:  88.0\n",
            "Mean Reward:  97.1010101010101 Steps 27300 number of episodes: 527 e: 0.05\n",
            "Episode Reward:  104.0\n",
            "Mean Reward:  96.85858585858585 Steps 27400 number of episodes: 528 e: 0.05\n",
            "Episode Reward:  75.0\n",
            "Mean Reward:  96.8989898989899 Steps 27500 number of episodes: 529 e: 0.05\n",
            "Episode Reward:  80.0\n",
            "Mean Reward:  96.44444444444444 Steps 27600 number of episodes: 530 e: 0.05\n",
            "Episode Reward:  70.0\n",
            "Episode Reward:  74.0\n",
            "Mean Reward:  96.14141414141415 Steps 27700 number of episodes: 532 e: 0.05\n",
            "Mean Reward:  96.14141414141415 Steps 27800 number of episodes: 532 e: 0.05\n",
            "Episode Reward:  155.0\n",
            "Mean Reward:  96.0 Steps 27900 number of episodes: 533 e: 0.05\n",
            "Mean Reward:  96.0 Steps 28000 number of episodes: 533 e: 0.05\n",
            "Episode Reward:  208.0\n",
            "Mean Reward:  96.75757575757575 Steps 28100 number of episodes: 534 e: 0.05\n",
            "Episode Reward:  80.0\n",
            "Mean Reward:  97.79797979797979 Steps 28200 number of episodes: 535 e: 0.05\n",
            "Episode Reward:  94.0\n",
            "Mean Reward:  97.8080808080808 Steps 28300 number of episodes: 536 e: 0.05\n",
            "Episode Reward:  112.0\n",
            "Mean Reward:  97.88888888888889 Steps 28400 number of episodes: 537 e: 0.05\n",
            "Episode Reward:  62.0\n",
            "Episode Reward:  70.0\n",
            "Mean Reward:  96.31313131313131 Steps 28500 number of episodes: 539 e: 0.05\n",
            "Mean Reward:  96.31313131313131 Steps 28600 number of episodes: 539 e: 0.05\n",
            "Mean Reward:  96.31313131313131 Steps 28700 number of episodes: 539 e: 0.05\n",
            "Episode Reward:  252.0\n",
            "Episode Reward:  60.0\n",
            "Mean Reward:  96.66666666666667 Steps 28800 number of episodes: 541 e: 0.05\n",
            "Episode Reward:  56.0\n",
            "Mean Reward:  95.57575757575758 Steps 28900 number of episodes: 542 e: 0.05\n",
            "Episode Reward:  76.0\n",
            "Episode Reward:  68.0\n",
            "Mean Reward:  95.58585858585859 Steps 29000 number of episodes: 544 e: 0.05\n",
            "Episode Reward:  73.0\n",
            "Mean Reward:  95.48484848484848 Steps 29100 number of episodes: 545 e: 0.05\n",
            "Mean Reward:  95.48484848484848 Steps 29200 number of episodes: 545 e: 0.05\n",
            "Episode Reward:  152.0\n",
            "Episode Reward:  78.0\n",
            "Mean Reward:  95.73737373737374 Steps 29300 number of episodes: 547 e: 0.05\n",
            "Episode Reward:  88.0\n",
            "Mean Reward:  95.67676767676768 Steps 29400 number of episodes: 548 e: 0.05\n",
            "Episode Reward:  65.0\n",
            "Mean Reward:  95.8989898989899 Steps 29500 number of episodes: 549 e: 0.05\n",
            "Episode Reward:  62.0\n",
            "Episode Reward:  98.0\n",
            "Mean Reward:  95.81818181818181 Steps 29600 number of episodes: 551 e: 0.05\n",
            "Episode Reward:  62.0\n",
            "Mean Reward:  95.74747474747475 Steps 29700 number of episodes: 552 e: 0.05\n",
            "Episode Reward:  100.0\n",
            "Mean Reward:  95.8989898989899 Steps 29800 number of episodes: 553 e: 0.05\n",
            "Mean Reward:  95.8989898989899 Steps 29900 number of episodes: 553 e: 0.05\n",
            "Episode Reward:  143.0\n",
            "Episode Reward:  72.0\n",
            "Mean Reward:  96.85858585858585 Steps 30000 number of episodes: 555 e: 0.05\n",
            "Episode Reward:  76.0\n",
            "Mean Reward:  95.98989898989899 Steps 30100 number of episodes: 556 e: 0.05\n",
            "Mean Reward:  95.98989898989899 Steps 30200 number of episodes: 556 e: 0.05\n",
            "Episode Reward:  216.0\n",
            "Mean Reward:  96.02020202020202 Steps 30300 number of episodes: 557 e: 0.05\n",
            "Mean Reward:  96.02020202020202 Steps 30400 number of episodes: 557 e: 0.05\n",
            "Mean Reward:  96.02020202020202 Steps 30500 number of episodes: 557 e: 0.05\n",
            "Episode Reward:  304.0\n",
            "Mean Reward:  97.21212121212122 Steps 30600 number of episodes: 558 e: 0.05\n",
            "Episode Reward:  68.0\n",
            "Mean Reward:  99.15151515151516 Steps 30700 number of episodes: 559 e: 0.05\n",
            "Mean Reward:  99.15151515151516 Steps 30800 number of episodes: 559 e: 0.05\n",
            "Mean Reward:  99.15151515151516 Steps 30900 number of episodes: 559 e: 0.05\n",
            "Episode Reward:  313.0\n",
            "Mean Reward:  98.75757575757575 Steps 31000 number of episodes: 560 e: 0.05\n",
            "Mean Reward:  98.75757575757575 Steps 31100 number of episodes: 560 e: 0.05\n",
            "Episode Reward:  226.0\n",
            "Mean Reward:  101.0 Steps 31200 number of episodes: 561 e: 0.05\n",
            "Episode Reward:  73.0\n",
            "Mean Reward:  102.76767676767676 Steps 31300 number of episodes: 562 e: 0.05\n",
            "Episode Reward:  83.0\n",
            "Mean Reward:  102.57575757575758 Steps 31400 number of episodes: 563 e: 0.05\n",
            "Episode Reward:  84.0\n",
            "Mean Reward:  102.78787878787878 Steps 31500 number of episodes: 564 e: 0.05\n",
            "Episode Reward:  114.0\n",
            "Mean Reward:  102.93939393939394 Steps 31600 number of episodes: 565 e: 0.05\n",
            "Episode Reward:  86.0\n",
            "Episode Reward:  71.0\n",
            "Mean Reward:  102.02020202020202 Steps 31700 number of episodes: 567 e: 0.05\n",
            "Episode Reward:  100.0\n",
            "Mean Reward:  101.8080808080808 Steps 31800 number of episodes: 568 e: 0.05\n",
            "Episode Reward:  65.0\n",
            "Mean Reward:  101.36363636363636 Steps 31900 number of episodes: 569 e: 0.05\n",
            "Episode Reward:  66.0\n",
            "Mean Reward:  100.76767676767676 Steps 32000 number of episodes: 570 e: 0.05\n",
            "Mean Reward:  100.76767676767676 Steps 32100 number of episodes: 570 e: 0.05\n",
            "Episode Reward:  181.0\n",
            "Episode Reward:  91.0\n",
            "Mean Reward:  101.68686868686869 Steps 32200 number of episodes: 572 e: 0.05\n",
            "Mean Reward:  101.68686868686869 Steps 32300 number of episodes: 572 e: 0.05\n",
            "Episode Reward:  199.0\n",
            "Mean Reward:  101.8989898989899 Steps 32400 number of episodes: 573 e: 0.05\n",
            "Mean Reward:  101.8989898989899 Steps 32500 number of episodes: 573 e: 0.05\n",
            "Mean Reward:  101.8989898989899 Steps 32600 number of episodes: 573 e: 0.05\n",
            "Episode Reward:  285.0\n",
            "Mean Reward:  102.95959595959596 Steps 32700 number of episodes: 574 e: 0.05\n",
            "Mean Reward:  102.95959595959596 Steps 32800 number of episodes: 574 e: 0.05\n",
            "Mean Reward:  102.95959595959596 Steps 32900 number of episodes: 574 e: 0.05\n",
            "Episode Reward:  320.0\n",
            "Mean Reward:  104.83838383838383 Steps 33000 number of episodes: 575 e: 0.05\n",
            "Episode Reward:  84.0\n",
            "Mean Reward:  107.41414141414141 Steps 33100 number of episodes: 576 e: 0.05\n",
            "Episode Reward:  71.0\n",
            "Mean Reward:  107.16161616161617 Steps 33200 number of episodes: 577 e: 0.05\n",
            "Episode Reward:  60.0\n",
            "Mean Reward:  107.11111111111111 Steps 33300 number of episodes: 578 e: 0.05\n",
            "Mean Reward:  107.11111111111111 Steps 33400 number of episodes: 578 e: 0.05\n",
            "Episode Reward:  256.0\n",
            "Mean Reward:  106.84848484848484 Steps 33500 number of episodes: 579 e: 0.05\n",
            "Episode Reward:  79.0\n",
            "Mean Reward:  108.42424242424242 Steps 33600 number of episodes: 580 e: 0.05\n",
            "Episode Reward:  100.0\n",
            "Mean Reward:  108.04040404040404 Steps 33700 number of episodes: 581 e: 0.05\n",
            "Episode Reward:  92.0\n",
            "Mean Reward:  108.33333333333333 Steps 33800 number of episodes: 582 e: 0.05\n",
            "Mean Reward:  108.33333333333333 Steps 33900 number of episodes: 582 e: 0.05\n",
            "Mean Reward:  108.33333333333333 Steps 34000 number of episodes: 582 e: 0.05\n",
            "Episode Reward:  299.0\n",
            "Mean Reward:  108.57575757575758 Steps 34100 number of episodes: 583 e: 0.05\n",
            "Mean Reward:  108.57575757575758 Steps 34200 number of episodes: 583 e: 0.05\n",
            "Mean Reward:  108.57575757575758 Steps 34300 number of episodes: 583 e: 0.05\n",
            "Episode Reward:  279.0\n",
            "Mean Reward:  110.66666666666667 Steps 34400 number of episodes: 584 e: 0.05\n",
            "Mean Reward:  110.66666666666667 Steps 34500 number of episodes: 584 e: 0.05\n",
            "Episode Reward:  271.0\n",
            "Mean Reward:  112.3030303030303 Steps 34600 number of episodes: 585 e: 0.05\n",
            "Mean Reward:  112.3030303030303 Steps 34700 number of episodes: 585 e: 0.05\n",
            "Mean Reward:  112.3030303030303 Steps 34800 number of episodes: 585 e: 0.05\n",
            "Episode Reward:  217.0\n",
            "Mean Reward:  114.0909090909091 Steps 34900 number of episodes: 586 e: 0.05\n",
            "Episode Reward:  95.0\n",
            "Mean Reward:  115.4040404040404 Steps 35000 number of episodes: 587 e: 0.05\n",
            "Mean Reward:  115.4040404040404 Steps 35100 number of episodes: 587 e: 0.05\n",
            "Episode Reward:  261.0\n",
            "Mean Reward:  115.64646464646465 Steps 35200 number of episodes: 588 e: 0.05\n",
            "Mean Reward:  115.64646464646465 Steps 35300 number of episodes: 588 e: 0.05\n",
            "Mean Reward:  115.64646464646465 Steps 35400 number of episodes: 588 e: 0.05\n",
            "Episode Reward:  241.0\n",
            "Mean Reward:  117.31313131313131 Steps 35500 number of episodes: 589 e: 0.05\n",
            "Mean Reward:  117.31313131313131 Steps 35600 number of episodes: 589 e: 0.05\n",
            "Mean Reward:  117.31313131313131 Steps 35700 number of episodes: 589 e: 0.05\n",
            "Episode Reward:  299.0\n",
            "Mean Reward:  118.79797979797979 Steps 35800 number of episodes: 590 e: 0.05\n",
            "Mean Reward:  118.79797979797979 Steps 35900 number of episodes: 590 e: 0.05\n",
            "Episode Reward:  297.0\n",
            "Mean Reward:  121.0909090909091 Steps 36000 number of episodes: 591 e: 0.05\n",
            "Episode Reward:  36.0\n",
            "Mean Reward:  123.20202020202021 Steps 36100 number of episodes: 592 e: 0.05\n",
            "Episode Reward:  150.0\n",
            "Mean Reward:  123.01010101010101 Steps 36200 number of episodes: 593 e: 0.05\n",
            "Episode Reward:  100.0\n",
            "Mean Reward:  123.53535353535354 Steps 36300 number of episodes: 594 e: 0.05\n",
            "Mean Reward:  123.53535353535354 Steps 36400 number of episodes: 594 e: 0.05\n",
            "Mean Reward:  123.53535353535354 Steps 36500 number of episodes: 594 e: 0.05\n",
            "Episode Reward:  263.0\n",
            "Mean Reward:  123.70707070707071 Steps 36600 number of episodes: 595 e: 0.05\n",
            "Episode Reward:  116.0\n",
            "Mean Reward:  125.4040404040404 Steps 36700 number of episodes: 596 e: 0.05\n",
            "Mean Reward:  125.4040404040404 Steps 36800 number of episodes: 596 e: 0.05\n",
            "Episode Reward:  144.0\n",
            "Mean Reward:  125.78787878787878 Steps 36900 number of episodes: 597 e: 0.05\n",
            "Mean Reward:  125.78787878787878 Steps 37000 number of episodes: 597 e: 0.05\n",
            "Mean Reward:  125.78787878787878 Steps 37100 number of episodes: 597 e: 0.05\n",
            "Episode Reward:  302.0\n",
            "Mean Reward:  125.13131313131314 Steps 37200 number of episodes: 598 e: 0.05\n",
            "Mean Reward:  125.13131313131314 Steps 37300 number of episodes: 598 e: 0.05\n",
            "Mean Reward:  125.13131313131314 Steps 37400 number of episodes: 598 e: 0.05\n",
            "Episode Reward:  306.0\n",
            "Mean Reward:  127.11111111111111 Steps 37500 number of episodes: 599 e: 0.05\n",
            "Mean Reward:  127.11111111111111 Steps 37600 number of episodes: 599 e: 0.05\n",
            "Episode Reward:  270.0\n",
            "Mean Reward:  129.13131313131314 Steps 37700 number of episodes: 600 e: 0.05\n",
            "Mean Reward:  129.13131313131314 Steps 37800 number of episodes: 600 e: 0.05\n",
            "Mean Reward:  129.13131313131314 Steps 37900 number of episodes: 600 e: 0.05\n",
            "Episode Reward:  265.0\n",
            "Mean Reward:  131.1818181818182 Steps 38000 number of episodes: 601 e: 0.05\n",
            "Episode Reward:  84.0\n",
            "Mean Reward:  133.010101010101 Steps 38100 number of episodes: 602 e: 0.05\n",
            "Mean Reward:  133.010101010101 Steps 38200 number of episodes: 602 e: 0.05\n",
            "Mean Reward:  133.010101010101 Steps 38300 number of episodes: 602 e: 0.05\n",
            "Episode Reward:  285.0\n",
            "Mean Reward:  133.06060606060606 Steps 38400 number of episodes: 603 e: 0.05\n",
            "Episode Reward:  96.0\n",
            "Mean Reward:  135.0 Steps 38500 number of episodes: 604 e: 0.05\n",
            "Mean Reward:  135.0 Steps 38600 number of episodes: 604 e: 0.05\n",
            "Mean Reward:  135.0 Steps 38700 number of episodes: 604 e: 0.05\n",
            "Mean Reward:  135.0 Steps 38800 number of episodes: 604 e: 0.05\n",
            "Episode Reward:  427.0\n",
            "Mean Reward:  135.2929292929293 Steps 38900 number of episodes: 605 e: 0.05\n",
            "Episode Reward:  82.0\n",
            "Mean Reward:  138.7979797979798 Steps 39000 number of episodes: 606 e: 0.05\n",
            "Episode Reward:  84.0\n",
            "Mean Reward:  138.72727272727272 Steps 39100 number of episodes: 607 e: 0.05\n",
            "Mean Reward:  138.72727272727272 Steps 39200 number of episodes: 607 e: 0.05\n",
            "Mean Reward:  138.72727272727272 Steps 39300 number of episodes: 607 e: 0.05\n",
            "Episode Reward:  329.0\n",
            "Mean Reward:  138.37373737373738 Steps 39400 number of episodes: 608 e: 0.05\n",
            "Mean Reward:  138.37373737373738 Steps 39500 number of episodes: 608 e: 0.05\n",
            "Mean Reward:  138.37373737373738 Steps 39600 number of episodes: 608 e: 0.05\n",
            "Episode Reward:  314.0\n",
            "Mean Reward:  140.13131313131314 Steps 39700 number of episodes: 609 e: 0.05\n",
            "Mean Reward:  140.13131313131314 Steps 39800 number of episodes: 609 e: 0.05\n",
            "Mean Reward:  140.13131313131314 Steps 39900 number of episodes: 609 e: 0.05\n",
            "Mean Reward:  140.13131313131314 Steps 40000 number of episodes: 609 e: 0.05\n",
            "Episode Reward:  393.0\n",
            "Mean Reward:  141.78787878787878 Steps 40100 number of episodes: 610 e: 0.05\n",
            "Mean Reward:  141.78787878787878 Steps 40200 number of episodes: 610 e: 0.05\n",
            "Mean Reward:  141.78787878787878 Steps 40300 number of episodes: 610 e: 0.05\n",
            "Episode Reward:  304.0\n",
            "Mean Reward:  144.94949494949495 Steps 40400 number of episodes: 611 e: 0.05\n",
            "Mean Reward:  144.94949494949495 Steps 40500 number of episodes: 611 e: 0.05\n",
            "Mean Reward:  144.94949494949495 Steps 40600 number of episodes: 611 e: 0.05\n",
            "Mean Reward:  144.94949494949495 Steps 40700 number of episodes: 611 e: 0.05\n",
            "Episode Reward:  395.0\n",
            "Mean Reward:  147.1010101010101 Steps 40800 number of episodes: 612 e: 0.05\n",
            "Mean Reward:  147.1010101010101 Steps 40900 number of episodes: 612 e: 0.05\n",
            "Episode Reward:  213.0\n",
            "Mean Reward:  150.17171717171718 Steps 41000 number of episodes: 613 e: 0.05\n",
            "Mean Reward:  150.17171717171718 Steps 41100 number of episodes: 613 e: 0.05\n",
            "Mean Reward:  150.17171717171718 Steps 41200 number of episodes: 613 e: 0.05\n",
            "Mean Reward:  150.17171717171718 Steps 41300 number of episodes: 613 e: 0.05\n",
            "Mean Reward:  150.17171717171718 Steps 41400 number of episodes: 613 e: 0.05\n",
            "Episode Reward:  500.0\n",
            "Mean Reward:  151.36363636363637 Steps 41500 number of episodes: 614 e: 0.05\n",
            "Episode Reward:  77.0\n",
            "Mean Reward:  155.65656565656565 Steps 41600 number of episodes: 615 e: 0.05\n",
            "Episode Reward:  123.0\n",
            "Mean Reward:  155.5151515151515 Steps 41700 number of episodes: 616 e: 0.05\n",
            "Mean Reward:  155.5151515151515 Steps 41800 number of episodes: 616 e: 0.05\n",
            "Episode Reward:  173.0\n",
            "Mean Reward:  155.82828282828282 Steps 41900 number of episodes: 617 e: 0.05\n",
            "Mean Reward:  155.82828282828282 Steps 42000 number of episodes: 617 e: 0.05\n",
            "Mean Reward:  155.82828282828282 Steps 42100 number of episodes: 617 e: 0.05\n",
            "Mean Reward:  155.82828282828282 Steps 42200 number of episodes: 617 e: 0.05\n",
            "Mean Reward:  155.82828282828282 Steps 42300 number of episodes: 617 e: 0.05\n",
            "Episode Reward:  500.0\n",
            "Mean Reward:  156.72727272727272 Steps 42400 number of episodes: 618 e: 0.05\n",
            "Mean Reward:  156.72727272727272 Steps 42500 number of episodes: 618 e: 0.05\n",
            "Mean Reward:  156.72727272727272 Steps 42600 number of episodes: 618 e: 0.05\n",
            "Mean Reward:  156.72727272727272 Steps 42700 number of episodes: 618 e: 0.05\n",
            "Mean Reward:  156.72727272727272 Steps 42800 number of episodes: 618 e: 0.05\n",
            "Episode Reward:  500.0\n",
            "Mean Reward:  160.96969696969697 Steps 42900 number of episodes: 619 e: 0.05\n",
            "Episode Reward:  97.0\n",
            "Mean Reward:  165.35353535353536 Steps 43000 number of episodes: 620 e: 0.05\n",
            "Mean Reward:  165.35353535353536 Steps 43100 number of episodes: 620 e: 0.05\n",
            "Mean Reward:  165.35353535353536 Steps 43200 number of episodes: 620 e: 0.05\n",
            "Mean Reward:  165.35353535353536 Steps 43300 number of episodes: 620 e: 0.05\n",
            "Mean Reward:  165.35353535353536 Steps 43400 number of episodes: 620 e: 0.05\n",
            "Episode Reward:  500.0\n",
            "Mean Reward:  164.8080808080808 Steps 43500 number of episodes: 621 e: 0.05\n",
            "Mean Reward:  164.8080808080808 Steps 43600 number of episodes: 621 e: 0.05\n",
            "Mean Reward:  164.8080808080808 Steps 43700 number of episodes: 621 e: 0.05\n",
            "Mean Reward:  164.8080808080808 Steps 43800 number of episodes: 621 e: 0.05\n",
            "Mean Reward:  164.8080808080808 Steps 43900 number of episodes: 621 e: 0.05\n",
            "Episode Reward:  500.0\n",
            "Mean Reward:  169.1818181818182 Steps 44000 number of episodes: 622 e: 0.05\n",
            "Episode Reward:  112.0\n",
            "Mean Reward:  171.96969696969697 Steps 44100 number of episodes: 623 e: 0.05\n",
            "Mean Reward:  171.96969696969697 Steps 44200 number of episodes: 623 e: 0.05\n",
            "Mean Reward:  171.96969696969697 Steps 44300 number of episodes: 623 e: 0.05\n",
            "Mean Reward:  171.96969696969697 Steps 44400 number of episodes: 623 e: 0.05\n",
            "Mean Reward:  171.96969696969697 Steps 44500 number of episodes: 623 e: 0.05\n",
            "Episode Reward:  500.0\n",
            "Mean Reward:  172.03030303030303 Steps 44600 number of episodes: 624 e: 0.05\n",
            "Mean Reward:  172.03030303030303 Steps 44700 number of episodes: 624 e: 0.05\n",
            "Mean Reward:  172.03030303030303 Steps 44800 number of episodes: 624 e: 0.05\n",
            "Mean Reward:  172.03030303030303 Steps 44900 number of episodes: 624 e: 0.05\n",
            "Mean Reward:  172.03030303030303 Steps 45000 number of episodes: 624 e: 0.05\n",
            "Episode Reward:  500.0\n",
            "Mean Reward:  176.35353535353536 Steps 45100 number of episodes: 625 e: 0.05\n",
            "Mean Reward:  176.35353535353536 Steps 45200 number of episodes: 625 e: 0.05\n",
            "Mean Reward:  176.35353535353536 Steps 45300 number of episodes: 625 e: 0.05\n",
            "Mean Reward:  176.35353535353536 Steps 45400 number of episodes: 625 e: 0.05\n",
            "Mean Reward:  176.35353535353536 Steps 45500 number of episodes: 625 e: 0.05\n",
            "Episode Reward:  500.0\n",
            "Mean Reward:  180.1919191919192 Steps 45600 number of episodes: 626 e: 0.05\n",
            "Mean Reward:  180.1919191919192 Steps 45700 number of episodes: 626 e: 0.05\n",
            "Mean Reward:  180.1919191919192 Steps 45800 number of episodes: 626 e: 0.05\n",
            "Mean Reward:  180.1919191919192 Steps 45900 number of episodes: 626 e: 0.05\n",
            "Mean Reward:  180.1919191919192 Steps 46000 number of episodes: 626 e: 0.05\n",
            "Episode Reward:  500.0\n",
            "Mean Reward:  184.35353535353536 Steps 46100 number of episodes: 627 e: 0.05\n",
            "Mean Reward:  184.35353535353536 Steps 46200 number of episodes: 627 e: 0.05\n",
            "Mean Reward:  184.35353535353536 Steps 46300 number of episodes: 627 e: 0.05\n",
            "Mean Reward:  184.35353535353536 Steps 46400 number of episodes: 627 e: 0.05\n",
            "Mean Reward:  184.35353535353536 Steps 46500 number of episodes: 627 e: 0.05\n",
            "Episode Reward:  500.0\n",
            "Mean Reward:  188.35353535353536 Steps 46600 number of episodes: 628 e: 0.05\n",
            "Episode Reward:  152.0\n",
            "Mean Reward:  192.64646464646464 Steps 46700 number of episodes: 629 e: 0.05\n",
            "Mean Reward:  192.64646464646464 Steps 46800 number of episodes: 629 e: 0.05\n",
            "Mean Reward:  192.64646464646464 Steps 46900 number of episodes: 629 e: 0.05\n",
            "Mean Reward:  192.64646464646464 Steps 47000 number of episodes: 629 e: 0.05\n",
            "Mean Reward:  192.64646464646464 Steps 47100 number of episodes: 629 e: 0.05\n",
            "Episode Reward:  500.0\n",
            "Mean Reward:  193.37373737373738 Steps 47200 number of episodes: 630 e: 0.05\n",
            "Mean Reward:  193.37373737373738 Steps 47300 number of episodes: 630 e: 0.05\n",
            "Episode Reward:  125.0\n",
            "Mean Reward:  197.7171717171717 Steps 47400 number of episodes: 631 e: 0.05\n",
            "We Are DONE Training!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEtPD0wceiEL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5856dcca-0b29-4555-a49f-c0444e0b61b3"
      },
      "source": [
        " import gym\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "observation = env.reset()\n",
        "print(env.action_space)\n",
        "for _ in range(100000):\n",
        "  #env.render()\n",
        "  action = env.action_space.sample() # your agent here (this takes random actions)\n",
        "  observation, reward, done, info = env.step(action)\n",
        "\n",
        "  if done:\n",
        "    observation = env.reset()\n",
        "env.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(2)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}