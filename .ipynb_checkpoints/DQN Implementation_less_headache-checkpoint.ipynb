{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.nn.functional as F           # layers, activations and more\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "import gym\n",
    "from collections import deque\n",
    "import collections, itertools\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self,obs_array_len, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc =nn.Linear(obs_array_len,512)\n",
    "        self.output = nn.Linear(512, num_actions) #according to env.action_space, there are 4 actions that can be taken\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        x=F.relu(self.fc(obs))\n",
    "        x=self.output(x)\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#named tupule creates a 'class' that stores state action reward... we can then create a list or queue to store instances of the class\n",
    "Experience = collections.namedtuple('Experience',['state', 'action', 'reward', 'done', 'state1']) #done is important for when we calculate the losses\n",
    "\n",
    "class Experience_replay(): #experience replay datatype\n",
    "    def __init__(self, REPLAY_SIZE):\n",
    "        self.memory= collections.deque(maxlen = REPLAY_SIZE) #deque with max size given\n",
    "\n",
    "    def insert(self, experience):\n",
    "        self.memory.append(experience)\n",
    "        \n",
    "    def size(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def sample(self, batch_size): #return a batch to calculate loss\n",
    "        random_indexes = np.random.choice(len(self.memory), batch_size, replace = False) #create list of random\n",
    "        \n",
    "        states, actions, rewards, dones, state1s = zip(* [self.memory[index] for index in random_indexes])\n",
    "        \n",
    "        states = np.array(states)                \n",
    "        actions = np.array(actions)                \n",
    "        rewards = np.array(rewards, dtype=np.float32)        \n",
    "        dones = np.array(dones, dtype=np.uint8)        \n",
    "        state1s = np.array(state1s)\n",
    "        \n",
    "        return states, actions, rewards, dones, state1s\n",
    "            \n",
    "def to_torch(i):\n",
    "    return torch.from_numpy(i)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb=SummaryWriter(\"runs/Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    #enable cuda\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    \n",
    "    #hyperparameters\n",
    "\n",
    "    GAMMA=0.99                #Gamma for bellman approx.\n",
    "    BATCH_SIZE=32             #size of batch to sample from replay memory\n",
    "    REPLAY_SIZE=10000         #size of replay memory\n",
    "    LR=1e-4                   #learning rate\n",
    "    EPSILON_START=1.0         #exploration \n",
    "    EPSILON_FINAL=0.1\n",
    "    EPSILON_DECAY_LENGTH=150000\n",
    "    epsilon=EPSILON_START\n",
    "    count =0\n",
    "    \n",
    "    avg_reward=collections.deque(maxlen = 100)\n",
    "    \n",
    "    #make environment\n",
    "    \n",
    "    env = gym.make('Breakout-ram-v0')\n",
    "    obs=env.reset()\n",
    "    done = False\n",
    "    \n",
    "    #Network\n",
    "    \n",
    "    net=DQN(len(obs),2)       #define network\n",
    "    loss_fn=nn.MSELoss()\n",
    "    optimizer= optim.RMSprop(net.parameters(), lr=LR)\n",
    "    \n",
    "    mem_buffer = Experience_replay(REPLAY_SIZE)\n",
    "    \n",
    "    while (mem_buffer.size() < REPLAY_SIZE):    #play random actions to fill memory buffer\n",
    "        if done:\n",
    "            obs=env.reset()\n",
    "            done=False\n",
    "        action = env.action_space.sample()\n",
    "        obs1, reward, done, info = env.step(action)\n",
    "        experience = Experience(obs, action, reward, done, obs1)\n",
    "        obs = obs1\n",
    "        mem_buffer.insert(experience)\n",
    "    \n",
    "    \n",
    "    inp=torch.empty(0)     #initilize empty tensor(no values at all)\n",
    "    target=torch.empty(0)  #can use torch.randn(0) too\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if (count>=EPSILON_DECAY_LENGTH):\n",
    "            print(\"we have reached final decay\")\n",
    "        \n",
    "        count+=1\n",
    "        epsilon = max(EPSILON_FINAL, EPSILON_START - count/EPSILON_DECAY_LENGTH) #decay epsilon\n",
    "        \n",
    "        #np.random.random() returns random floats in the half-open interval [0.0, 1.0). \n",
    "        #with epsilon getting smaller, theres less chance that the random number will be within the range of epsilon\n",
    "        \n",
    "        if np.random.random() < epsilon:   \n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        else:\n",
    "            action= torch.argmax(net(to_torch(obs).float())) #output action.            \n",
    "        \n",
    "        \n",
    "        obs1, reward, done, info = env.step(action)\n",
    "        \n",
    "        experience = Experience(obs, action, reward, done, obs1) #create transition\n",
    "        obs = obs1       #update observation for next time step\n",
    "        mem_buffer.insert(experience) #store transition in memory buffer\n",
    "        \n",
    "        if done:\n",
    "            obs=env.reset()\n",
    "            done=False\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        #Record training performance to tensorboard\n",
    "        avg_reward.append(reward)\n",
    "        \n",
    "        if(count%1000==0):\n",
    "            \n",
    "            mean_reward=np.mean(list(itertools.islice(avg_reward, 0, 99)))\n",
    "            tb.add_scalar('Mean Reward', mean_reward, count)\n",
    "        \n",
    "        #sample from minibatch\n",
    "        \n",
    "        obs_batch, action_batch, reward_batch,done_batch, obs1_batch= mem_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "        for i in range(len(obs_batch)):\n",
    "            if done_batch[i] == True:\n",
    "                inp= torch.cat((inp, torch.unsqueeze(to_torch(np.array(reward_batch[i])), 0) ),0) #for every reward at index i, concatenate to the end of tensor\n",
    "            else:\n",
    "                #unsqueeze Returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "                inp= torch.cat((inp, torch.unsqueeze(to_torch(np.array(reward_batch[i])), 0) + GAMMA*torch.unsqueeze(torch.max(net(to_torch(np.array(obs_batch[i])).float())), 0)),  0) \n",
    "            target= torch.cat((target, torch.unsqueeze(torch.max(net(to_torch(obs1_batch[i]).float())),0) ), 0)\n",
    "            \n",
    "        \n",
    "        loss=loss_fn(inp,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        inp=torch.empty(0)     #empty tensors to use again in loop\n",
    "        target=torch.empty(0)      \n",
    "        \n",
    "        \n",
    "        #save model\n",
    "        PATH=\"breakout-model.pth\"\n",
    "        \n",
    "        if(count%10000==0):\n",
    "            torch.save({\n",
    "                'epoch':count,\n",
    "                'model_state_dict': net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss':loss\n",
    "            },\n",
    "            PATH)        \n",
    "        \n",
    "    \n",
    "    \n",
    "    env.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-9a84f54f7328>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m#sample from minibatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mobs_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs1_batch\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmem_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-799eddc86e84>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate1s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrandom_indexes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(0)\n",
    "y= torch.randn(1)\n",
    "z = torch.randn(1)\n",
    "GAMMA=3\n",
    "a= torch.tensor(2)\n",
    "a= torch.unsqueeze(a,-1)\n",
    "\n",
    "x=torch.cat((x,y,z),0)\n",
    "print (x)\n",
    "print(a*GAMMA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
