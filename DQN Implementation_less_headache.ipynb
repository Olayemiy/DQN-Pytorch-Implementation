{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.nn.functional as F           # layers, activations and more\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "import gym\n",
    "from collections import deque\n",
    "import collections, itertools\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if training with google colab\n",
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "!ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
    "!ls /mydrive\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self,obs_array_len, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc =nn.Linear(obs_array_len,256)\n",
    "        self.fc1= nn.Linear(256,256)\n",
    "        self.output = nn.Linear(256, num_actions) #according to env.action_space, there are 4 actions that can be taken\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        x=F.relu(self.fc(obs))\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=self.output(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#named tupule creates a 'class' that stores state action reward... we can then create a list or queue to store instances of the class\n",
    "Experience = collections.namedtuple('Experience',['state', 'action', 'reward', 'done', 'state1']) #done is important for when we calculate the losses\n",
    "\n",
    "class Experience_replay(): #experience replay datatype\n",
    "    def __init__(self, REPLAY_SIZE):\n",
    "        self.memory= collections.deque(maxlen = REPLAY_SIZE) #deque with max size given\n",
    "\n",
    "    def insert(self, experience):\n",
    "        self.memory.append(experience)\n",
    "        \n",
    "    def size(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def sample(self, batch_size): #return a batch to calculate loss\n",
    "        random_indexes = np.random.choice(len(self.memory), batch_size, replace = False) #create list of random\n",
    "        \n",
    "        states, actions, rewards, dones, state1s = zip(* [self.memory[index] for index in random_indexes])\n",
    "        \n",
    "        states = np.array(states)                \n",
    "        actions = np.array(actions)                \n",
    "        rewards = np.array(rewards, dtype=np.float32)        \n",
    "        dones = np.array(dones, dtype=np.uint8)        \n",
    "        state1s = np.array(state1s)\n",
    "        \n",
    "        return states, actions, rewards, dones, state1s\n",
    "            \n",
    "def to_torch(i):\n",
    "    return torch.from_numpy(i)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb=SummaryWriter(\"runs/Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    #enable cuda\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    \n",
    "    #hyperparameters\n",
    "\n",
    "    GAMMA=0.99                #Gamma for bellman approx.\n",
    "    BATCH_SIZE=32             #size of batch to sample from replay memory\n",
    "    REPLAY_SIZE=10000         #size of replay memory\n",
    "    LR=1e-4                   #learning rate\n",
    "    EPSILON_START=1.0         #exploration \n",
    "    EPSILON_FINAL=0.1\n",
    "    EPSILON_DECAY_LENGTH=150000\n",
    "    epsilon=EPSILON_START\n",
    "    count =0\n",
    "    episode_reward=0\n",
    "    \n",
    "    avg_reward=collections.deque(maxlen = 100)\n",
    "    \n",
    "    #make environment\n",
    "    \n",
    "    env = gym.make('Breakout-ram-v0')  \n",
    "    obs=env.reset()\n",
    "    done = False\n",
    "    \n",
    "    #Network\n",
    "    \n",
    "    net= DQN(len(obs),4).to(device)       #define networks\n",
    "    target_net= DQN(len(obs),4).to(device)\n",
    "    target_net.load_state_dict(net.state_dict())   #we want the same parameters\n",
    "    target_net.eval()                              #we want to eventuall copy the parameters of net, not optimize target_net\n",
    "\n",
    "    #Loss\n",
    "\n",
    "    loss_fn=nn.MSELoss()\n",
    "\n",
    "    #Optimizer\n",
    "\n",
    "    optimizer= optim.RMSprop(net.parameters(), lr=LR)\n",
    "    \n",
    "    mem_buffer = Experience_replay(REPLAY_SIZE)\n",
    "    \n",
    "    while (mem_buffer.size() < REPLAY_SIZE):    #play random actions to fill memory buffer\n",
    "        if done:\n",
    "            obs=env.reset()\n",
    "            done=False\n",
    "        action = env.action_space.sample()\n",
    "        obs1, reward, done, info = env.step(action)\n",
    "        experience = Experience(obs, action, reward, done, obs1)\n",
    "        obs = obs1\n",
    "        mem_buffer.insert(experience)\n",
    "    \n",
    "    \n",
    "    inp=torch.empty(0)    #initilize empty tensor(no values at all)\n",
    "    target=torch.empty(0)  #can use torch.randn(0) too\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if (count==EPSILON_DECAY_LENGTH):\n",
    "            print(\"we have reached final decay\")\n",
    "        \n",
    "        count+=1\n",
    "        epsilon = max(EPSILON_FINAL, EPSILON_START - count/EPSILON_DECAY_LENGTH) #decay epsilon\n",
    "        \n",
    "        #np.random.random() returns random floats in the half-open interval [0.0, 1.0). \n",
    "        #with epsilon getting smaller, theres less chance that the random number will be within the range of epsilon\n",
    "        \n",
    "        if np.random.random() < epsilon:   \n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        else:\n",
    "            action= torch.argmax(net(to_torch(obs).to(device).float())) #output action.            \n",
    "        \n",
    "        \n",
    "        obs1, reward, done, info = env.step(action)\n",
    "        \n",
    "        experience = Experience(obs, action, reward, done, obs1) #create transition\n",
    "        obs = obs1       #update observation for next time step\n",
    "        mem_buffer.insert(experience) #store transition in memory buffer\n",
    "        \n",
    "        if done:\n",
    "            obs=env.reset()\n",
    "            done=False\n",
    "        \n",
    "        #env.render()\n",
    "        \n",
    "        #Record training performance to tensorboard---------------Average reward over 100 episodes\n",
    "        episode_reward+=reward\n",
    "        if done:\n",
    "          avg_reward.append(reward)\n",
    "          episode_reward=0\n",
    "        \n",
    "        if (count%5000==0):\n",
    "            mean_reward=np.mean(list(itertools.islice(avg_reward, 0, 99)))\n",
    "            tb.add_scalar('Mean Reward', mean_reward, count)\n",
    "            print(mean_reward, count)\n",
    "        \n",
    "        #sample from minibatch\n",
    "        \n",
    "        obs_batch, action_batch, reward_batch,done_batch, obs1_batch= mem_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "        for i in range(len(obs_batch)):\n",
    "            if done_batch[i] == True:\n",
    "                inp= torch.cat((inp.to(device), torch.unsqueeze(to_torch(np.array(reward_batch[i])).to(device), 0) ),0) #for every reward at index i, concatenate to the end of tensor\n",
    "            else:\n",
    "                #unsqueeze Returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "                inp= torch.cat((inp.to(device), torch.unsqueeze(to_torch(np.array(reward_batch[i])), 0).to(device) + GAMMA*torch.unsqueeze(torch.max(net(to_torch(np.array(obs_batch[i])).to(device).float())), 0)),  0) \n",
    "                inp= inp.to(device)\n",
    "            target= torch.cat((target.to(device), torch.unsqueeze(torch.max(target_net(to_torch(obs1_batch[i]).to(device).float())),0) ), 0)\n",
    "            \n",
    "        \n",
    "        loss=loss_fn(inp,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        inp=torch.empty(0)     #empty tensors to use again in loop\n",
    "        target=torch.empty(0)      \n",
    "        \n",
    "        \n",
    "        #save model\n",
    "        PATH=\"breakout-model.pth\"\n",
    "        PATH_for_target=\"breakout-target-model.pth\"\n",
    "        \n",
    "        if(count%10000==0):\n",
    "            torch.save({\n",
    "                'epoch':count,\n",
    "                'model_state_dict': net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss':loss\n",
    "            },\n",
    "            PATH)\n",
    "\n",
    "            target_net.load_state_dict(net.state_dict())\n",
    "            target_net.eval()        \n",
    "        \n",
    "    \n",
    "    \n",
    "    env.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-96ef5ab9cff2>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[1;31m#unsqueeze Returns a new tensor with a dimension of size one inserted at the specified position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 \u001b[0minp\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_torch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_torch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m                 \u001b[0minp\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_torch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs1_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(0)\n",
    "y= torch.randn(1)\n",
    "z = torch.randn(1)\n",
    "GAMMA=3\n",
    "a= torch.tensor(2)\n",
    "a= torch.unsqueeze(a,-1)\n",
    "\n",
    "x=torch.cat((x,y,z),0)\n",
    "print (x)\n",
    "print(a*GAMMA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python_defaultSpec_1596078581896"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}