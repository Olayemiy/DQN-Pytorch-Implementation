{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.nn.functional as F           # layers, activations and more\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "import gym\n",
    "from collections import deque\n",
    "import collections\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self,obs_array_len, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc =nn.Linear(obs_array_len,512)\n",
    "        self.output = nn.Linear(512, num_actions) #according to env.action_space, there are 4 actions that can be taken\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        x=F.relu(self.fc(obs))\n",
    "        x=self.output(x)\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#troubleshooting\n",
    "x=[0,1,2]  \n",
    "x=torch.from_numpy(np.array(x)).float()\n",
    "net=DQN(3,2)\n",
    "\n",
    "out= net(x)\n",
    "print (out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#named tupule creates a 'class' that stores state action reward... we can then create a list or queue to store instances of the class\n",
    "Experience = collections.namedtuple('Experience',['state', 'action', 'reward', 'done', 'state1']) #done is important for when we calculate the losses\n",
    "\n",
    "class Experience_replay(): #experience replay datatype\n",
    "    def __init__(self, REPLAY_SIZE):\n",
    "        self.memory= collections.deque(maxlen = REPLAY_SIZE) #deque with max size given\n",
    "\n",
    "    def insert(self, experience):\n",
    "        self.memory.append(experience)\n",
    "        \n",
    "    def size(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def sample(self, batch_size): #return a batch to calculate loss\n",
    "        random_indexes = np.random.choice(len(self.memory), batch_size, replace = False) #create list of random\n",
    "        \n",
    "        states, actions, rewards, dones, state1s = zip(* [self.memory[index] for index in random_indexes])\n",
    "        \n",
    "        states = np.array(states)                \n",
    "        actions = np.array(actions)                \n",
    "        rewards = np.array(rewards, dtype=np.float32)        \n",
    "        dones = np.array(dones, dtype=np.uint8)        \n",
    "        state1s = np.array(state1s)\n",
    "        \n",
    "        return states, actions, rewards, dones, state1s\n",
    "            \n",
    "def to_torch(i):\n",
    "    return torch.from_numpy(i)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    #hyperparameters\n",
    "    \n",
    "    count =0\n",
    "    \n",
    "    GAMMA=0.99                #Gamma for bellman approx.\n",
    "    BATCH_SIZE=32             #size of batch to sample from replay memory\n",
    "    REPLAY_SIZE=10000         #size of replay memory\n",
    "    LR=1e4                    #learning rate\n",
    "    EPSILON_START=1.0         #exploration \n",
    "    EPSILON_FINAL=0.1\n",
    "    EPSILON_DECAY_LENGTH=150000\n",
    "    epsilon=EPSILON_START\n",
    "\n",
    "    env = gym.make('Breakout-ram-v0')\n",
    "    obs=env.reset()\n",
    "    done = False\n",
    "    \n",
    "    net=DQN(len(obs),2)       #define network\n",
    "    loss_fn=nn.MSELoss()\n",
    "    optimizer= optim.RMSprop(net.parameters(), lr=LR)\n",
    "    \n",
    "    mem_buffer = Experience_replay(REPLAY_SIZE)\n",
    "    \n",
    "    while(mem_buffer.size() < REPLAY_SIZE):    #play random actions to fill memory buffer\n",
    "        if done:\n",
    "            obs=env.reset()\n",
    "            done=False\n",
    "        action = env.action_space.sample()\n",
    "        obs1, reward, done, info = env.step(action)\n",
    "        experience = Experience(obs, action, reward, done, obs1)\n",
    "        obs = obs1\n",
    "        mem_buffer.insert(experience)\n",
    "    \n",
    "    \n",
    "    inp=torch.empty(0)     #initilize empty tensor(no values at all)\n",
    "    target=torch.empty(0)  #can use torch.randn(0) too\n",
    "    \n",
    "    while True:\n",
    "        if (count>=EPSILON_DECAY_LENGTH):\n",
    "            print(\"we have reached final decay\")\n",
    "        \n",
    "        count+=1\n",
    "        epsilon = max(EPSILON_FINAL, EPSILON_START - count/EPSILON_DECAY_LENGTH) #decay epsilon\n",
    "        \n",
    "        #np.random.random() returns random floats in the half-open interval [0.0, 1.0). \n",
    "        #with epsilon getting smaller, theres less chance that the random number will be within the range of epsilon\n",
    "        \n",
    "        if np.random.random() < epsilon:   \n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        else:\n",
    "            obs = to_torch(obs).float()\n",
    "            action= torch.argmax(net(obs)) #output action.            \n",
    "        \n",
    "        \n",
    "        obs1, reward, done, info = env.step(action)\n",
    "        \n",
    "        experience = Experience(obs, action, reward, done, obs1) #create transition\n",
    "        obs = obs1       #update observation for next time step\n",
    "        mem_buffer.insert(experience) #store transition in memory buffer\n",
    "        \n",
    "        if done:\n",
    "            obs=env.reset()\n",
    "            done=False\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        #sample from minibatch\n",
    "        \n",
    "        obs_batch, action_batch, reward_batch,done_batch, obs1_batch= mem_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "        for i in range(len(obs_batch)):\n",
    "            if done_batch[i] == True:\n",
    "                inp= torch.cat((inp, torch.unsqueeze(to_torch(np.array(reward_batch[i])), 0) ),0) #for every reward at index i, concatenate to the end of tensor\n",
    "            else:\n",
    "                #unsqueeze Returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "                inp= torch.cat((inp, torch.unsqueeze(to_torch(np.array(reward_batch[i])), 0) + GAMMA*torch.unsqueeze(torch.max(net(to_torch(np.array(obs_batch[i])).float())), 0)),  0) \n",
    "            target= torch.cat((target, torch.unsqueeze(torch.max(net(to_torch(obs1_batch[i]).float())),0) ), 0)\n",
    "            \n",
    "        \n",
    "        loss=loss_fn(inp,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        inp=torch.empty(0)     #empty tensors to use again in loop\n",
    "        target=torch.empty(0)        \n",
    "        \n",
    "    \n",
    "    \n",
    "    env.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(0)\n",
    "y= torch.randn(1)\n",
    "z = torch.randn(1)\n",
    "GAMMA=3\n",
    "a= torch.tensor(2)\n",
    "a= torch.unsqueeze(a,-1)\n",
    "\n",
    "x=torch.cat((x,y,z),0)\n",
    "print (x)\n",
    "print(a*GAMMA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python_defaultSpec_1595988690538"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}